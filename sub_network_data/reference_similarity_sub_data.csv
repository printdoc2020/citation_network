Conference,Year,Title,DOI,id,cite_to_list,cited_by_list,Link,FirstPage,LastPage,PaperType,Abstract,AuthorNames-Deduped,AuthorNames,AuthorAffiliation,InternalReferences,AuthorKeywords,AminerCitationCount_02-2020,XploreCitationCount - 2020-01,PubsCited,Award
VAST,2008,Collaborative synthesis of visual analytic results,10.1109/VAST.2008.4677358,416,"[320, 329, 356]","[485, 524, 551, 607, 608, 675, 683, 865, 930, 941, 1040, 1043, 1232, 1464]",http://dx.doi.org/10.1109/VAST.2008.4677358,67,74,C,"Visual analytic tools allow analysts to generate large collections of useful analytical results. We anticipate that analysts in most real world situations will draw from these collections when working together to solve complicated problems. This indicates a need to understand how users synthesize multiple collections of results. This paper reports the results of collaborative synthesis experiments conducted with expert geographers and disease biologists. Ten participants were worked in pairs to complete a simulated real-world synthesis task using artifacts printed on cards on a large, paper-covered workspace. Experiment results indicate that groups use a number of different approaches to collaborative synthesis, and that they employ a variety of organizational metaphors to structure their information. It is further evident that establishing common ground and role assignment are critical aspects of collaborative synthesis. We conclude with a set of general design guidelines for collaborative synthesis support tools.",Anthony C. Robinson,Anthony C. Robinson,"GeoVISTA Center, Department of Geography, The Pennsylvania State University, USA",10.1109/VAST.2007.4389011;10.1109/TVCG.2007.70594;10.1109/TVCG.2007.70568,"Movement data, spatio-temporal data, aggregation, scalable visualization, geovisualization",65.0,41.0,17.0,
VAST,2008,Characterizing users' visual analytic activity for insight provenance,10.1109/VAST.2008.4677365,423,"[323, 294, 167, 107, 238, 176, 337, 21, 60]","[501, 502, 550, 551, 607, 609, 624, 683, 698, 761, 864, 868, 872, 880, 1040, 1054, 1172, 1179, 1181, 1278, 1288, 1370, 1375, 1415, 1458, 1486, 1559, 1565, 1566]",http://dx.doi.org/10.1109/VAST.2008.4677365,123,130,C,"Insight provenance - a historical record of the process and rationale by which an insight is derived - is an essential requirement in many visual analytics applications. While work in this area has relied on either manually recorded provenance (e.g., user notes) or automatically recorded event-based insight provenance (e.g., clicks, drags, and key-presses), both approaches have fundamental limitations. Our aim is to develop a new approach that combines the benefits of both approaches while avoiding their deficiencies. Toward this goal, we characterize userspsila visual analytic activity at multiple levels of granularity. Moreover, we identify a critical level of abstraction, Actions, that can be used to represent visual analytic activity with a set of general but semantically meaningful behavior types. In turn, the action types can be used as the semantic building blocks for insight provenance. We present a catalog of common actions identified through observations of several different visual analytic systems. In addition, we define a taxonomy to categorize actions into three major classes based on their semantic intent. The concept of actions has been integrated into our labpsilas prototype visual analytic system, HARVEST, as the basis for its insight provenance capabilities.",David Gotz;Michelle X. Zhou,David Gotz;Michelle X. Zhou,"IBM T.J. Watson Research Center, USA;IBM T.J. Watson Research Center, USA",10.1109/INFVIS.2004.2;10.1109/INFVIS.1996.559213;10.1109/TVCG.2007.70577;10.1109/VISUAL.2005.1532788;10.1109/VAST.2007.4388992;10.1109/INFVIS.2005.1532136;10.1109/INFVIS.1998.729560;10.1109/INFVIS.2004.10;10.1109/VAST.2006.261430;10.1109/INFVIS.2000.885092;10.1109/VISUAL.1990.146375;10.1109/VISUAL.2002.1183791,"Taxonomy, Information Visualization, Analytic Activity, Visual Analytics, Insight Provenance",125.0,34.0,29.0,
VAST,2009,Capturing and supporting the analysis process,10.1109/VAST.2009.5333020,501,"[420, 389, 423, 238, 176, 337, 307, 346, 347, 351]","[607, 692, 698, 738, 864, 947, 1045, 1172, 1177, 1179, 1273, 1278, 1464]",http://dx.doi.org/10.1109/VAST.2009.5333020,131,138,C,"Visual analytics tools provide powerful visual representations in order to support the sense-making process. In this process, analysts typically iterate through sequences of steps many times, varying parameters each time. Few visual analytics tools support this process well, nor do they provide support for visualizing and understanding the analysis process itself. To help analysts understand, explore, reference, and reuse their analysis process, we present a visual analytics system named CzSaw (See-Saw) that provides an editable and re-playable history navigation channel in addition to multiple visual representations of document collections and the entities within them (in a manner inspired by Jigsaw). Conventional history navigation tools range from basic undo and redo to branching timelines of user actions. In CzSaw's approach to this, first, user interactions are translated into a script language that drives the underlying scripting-driven propagation system. The latter allows analysts to edit analysis steps, and ultimately to program them. Second, on this base, we build both a history view showing progress and alternative paths, and a dependency graph showing the underlying logic of the analysis and dependency relations among the results of each step. These tools result in a visual model of the sense-making process, providing a way for analysts to visualize their analysis process, to reinterpret the problem, explore alternative paths, extract analysis patterns from existing history, and reuse them with other related analyses.",Nazanin Kadivar;Victor Y. Chen;Dustin Dunsmuir;Eric Lee;Cheryl Z. Qian;John Dill;Chris Shaw 0002;Robert F. Woodbury,Nazanin Kadivar;Victor Chen;Dustin Dunsmuir;Eric Lee;Cheryl Qian;John Dill;Christopher Shaw;Robert Woodbury,"School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada",10.1109/INFVIS.2005.1532136;10.1109/VAST.2008.4677362;10.1109/VAST.2007.4388992;10.1109/TVCG.2008.137;10.1109/VAST.2007.4389006;10.1109/VAST.2008.4677365;10.1109/INFVIS.2004.2;10.1109/VAST.2007.4389002;10.1109/TVCG.2007.70515;10.1109/VAST.2007.4389001,"Visual Analytics, Sense-making, Analysis Process, Visual History",54.0,33.0,28.0,
VAST,2009,Connecting the dots in visual analysis,10.1109/VAST.2009.5333023,502,"[356, 420, 423, 296, 351]","[607, 683, 738, 1040, 1185, 1288]",http://dx.doi.org/10.1109/VAST.2009.5333023,123,130,C,"During visual analysis, users must often connect insights discovered at various points of time. This process is often called ldquoconnecting the dots.rdquo When analysts interactively explore complex datasets over multiple sessions, they may uncover a large number of findings. As a result, it is often difficult for them to recall the past insights, views and concepts that are most relevant to their current line of inquiry. This challenge is even more difficult during collaborative analysis tasks where they need to find connections between their own discoveries and insights found by others. In this paper, we describe a context-based retrieval algorithm to identify notes, views and concepts from users' past analyses that are most relevant to a view or a note based on their line of inquiry. We then describe a related notes recommendation feature that surfaces the most relevant items to the user as they work based on this algorithm. We have implemented this recommendation feature in HARVEST, a Web based visual analytic system. We evaluate the related notes recommendation feature of HARVEST through a case study and discuss the implications of our approach.",Yedendra Babu Shrinivasan;David Gotz;Jie Lu,Yedendra B. Shrinivasan;David Gotzy;Jie Lu,"Eindhoven University of Technology, The Netherlands;IBM Research, USA;IBM Research, USA",10.1109/VAST.2008.4677362;10.1109/VAST.2007.4389006;10.1109/VAST.2007.4389011;10.1109/VAST.2006.261432;10.1109/VAST.2008.4677365,,28.0,12.0,20.0,
VAST,2010,A closer look at note taking in the co-located collaborative visual analytics process,10.1109/VAST.2010.5652879,607,"[416, 320, 323, 389, 423, 501, 502]","[683, 1040, 1288]",http://dx.doi.org/10.1109/VAST.2010.5652879,171,178,C,"This paper highlights the important role that record-keeping (i.e. taking notes and saving charts) plays in collaborative data analysis within the business domain. The discussion of record-keeping is based on observations from a user study in which co-located teams worked on collaborative visual analytics tasks using large interactive wall and tabletop displays. Part of our findings is a collaborative data analysis framework that encompasses note taking as one of the main activities. We observed that record-keeping was a critical activity within the analysis process. Based on our observations, we characterize notes according to their content, scope, and usage, and describe how they fit into a process of collaborative data analysis. We then discuss suggestions for the design of collaborative visual analytics tools.",Narges Mahyar;Ali Sarvghad;Melanie Tory,Narges Mahyar;Ali Sarvghad;Melanie Tory,University of Victoria;University of Victoria;University of Victoria,10.1109/TVCG.2008.137;10.1109/VAST.2008.4677358;10.1109/TVCG.2007.70568;10.1109/VAST.2009.5333020;10.1109/VAST.2008.4677365;10.1109/VAST.2009.5333023;10.1109/TVCG.2007.70577,"note taking, recording, collaboration, tabletop, wall display, history, provenance",14.0,8.0,33.0,
VAST,2011,Supporting effective common ground construction in Asynchronous Collaborative Visual Analytics,10.1109/VAST.2011.6102447,683,"[416, 609, 323, 356, 423, 617, 267, 502, 316, 607]","[738, 938, 1043, 1288, 1464, 1565]",http://dx.doi.org/10.1109/VAST.2011.6102447,101,110,C,"Asynchronous Collaborative Visual Analytics (ACVA) leverages group sensemaking by releasing the constraints on when, where, and who works collaboratively. A significant task to be addressed before ACVA can reach its full potential is effective common ground construction, namely the process in which users evaluate insights from individual work to develop a shared understanding of insights and collectively pool them. This is challenging due to the lack of instant communication and scale of collaboration in ACVA. We propose a novel visual analytics approach that automatically gathers, organizes, and summarizes insights to form common ground with reduced human effort. The rich set of visualization and interaction techniques provided in our approach allows users to effectively and flexibly control the common ground construction and review, explore, and compare insights in detail. A working prototype of the approach has been implemented. We have conducted a case study and a user study to demonstrate its effectiveness.",Yang Chen;Jamal Alsakran;Scott Barlowe;Jing Yang 0001;Ye Zhao,Yang Chen;Jamal Alsakran;Scott Barlowe;Jing Yang;Ye Zhao,"University of North Carolina at Charlotte, USA;Kent State University, USA;University of North Carolina at Charlotte, USA;University of North Carolina at Charlotte, USA;Kent State University, USA",10.1109/TVCG.2007.70541;10.1109/VAST.2009.5333023;10.1109/TVCG.2007.70577;10.1109/VAST.2010.5652879;10.1109/VAST.2008.4677358;10.1109/VAST.2010.5652932;10.1109/VAST.2007.4389011;10.1109/VAST.2008.4677365;10.1109/TVCG.2006.166;10.1109/VAST.2010.5652885,"Visual analytics, asynchronous collaboration, insight, multidimensional visualization",10.0,10.0,30.0,
VAST,2017,Beyond Tasks: An Activity Typology for Visual Analytics,10.1109/TVCG.2017.2745180,1458,"[1288, 1043, 1172, 675, 420, 294, 423, 424, 303, 323, 473, 351, 356, 872, 107, 238, 624, 755, 1275]",[],http://dx.doi.org/10.1109/TVCG.2017.2745180,267,277,J,"As Visual Analytics (VA) research grows and diversifies to encompass new systems, techniques, and use contexts, gaining a holistic view of analytic practices is becoming ever more challenging. However, such a view is essential for researchers and practitioners seeking to develop systems for broad audiences that span multiple domains. In this paper, we interpret VA research through the lens of Activity Theory (AT) - a framework for modelling human activities that has been influential in the field of Human-Computer Interaction. We first provide an overview of Activity Theory, showing its potential for thinking beyond tasks, representations, and interactions to the broader systems of activity in which interactive tools are embedded and used. Next, we describe how Activity Theory can be used as an organizing framework in the construction of activity typologies, building and expanding upon the tradition of abstract task taxonomies in the field of Information Visualization. We then apply the resulting process to create an activity typology for Visual Analytics, synthesizing a wide range of systems and activity concepts from the literature. Finally, we use this typology as the foundation of an activity-centered design process, highlighting both tensions and opportunities in the design space of VA systems.",Darren Edge;Nathalie Henry Riche;Jonathan Larson;Christopher White,Darren Edge;Nathalie Henry Riche;Jonathan Larson;Christopher White,Microsoft Research;Microsoft Research;Microsoft Research;Microsoft Research,10.1109/INFVIS.2005.1532136;10.1109/VAST.2008.4677362;10.1109/TVCG.2013.124;10.1109/VAST.2006.261439;10.1109/TVCG.2016.2598468;10.1109/INFVIS.2000.885092;10.1109/VAST.2006.261430;10.1109/VAST.2008.4677365;10.1109/VAST.2007.4389011;10.1109/VAST.2011.6102438;10.1109/VAST.2010.5653598;10.1109/TVCG.2014.2346573;10.1109/VAST.2008.4677366;10.1109/TVCG.2015.2467551;10.1109/TVCG.2012.213;10.1109/VAST.2007.4389006;10.1109/TVCG.2009.162;10.1109/TVCG.2007.70577;10.1109/TVCG.2016.2598543,"Activity theory,visual analytics,activity-centered design,literature review,human-computer interaction",2.0,1.0,77.0,
