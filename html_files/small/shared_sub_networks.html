<html>
<head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis-network.min.js"> </script>
<center>
<h2>Sub Networks</h2>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->

<style type="text/css">

        #mynetwork {
            width: 100%;
            height: 700px;
            background-color: #ffffff;
            border: 1px solid lightgray;
            position: relative;
            float: left;
        }

        

        

        
        /* position absolute is important and the container has to be relative or absolute as well. */
	    div.popup {
            position:absolute;
            top:0px;
            left:0px;
            display:none;
            background-color:#f5f4ed;
            -moz-border-radius: 3px;
            -webkit-border-radius: 3px;
            border-radius: 3px;
            border: 1px solid #808074;
            box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);
	    }

	    /* hide the original tooltip */
	    .vis-network-tooltip {
	      display:none;
	    }
        
</style>

</head>

<body>
<div id = "mynetwork"></div>


<script type="text/javascript">

    // initialize global variables.
    var edges;
    var nodes;
    var network; 
    var container;
    var options, data;

    
    // This method is responsible for drawing the graph, returns the drawn network
    function drawGraph() {
        var container = document.getElementById('mynetwork');
        
        

        // parsing and collecting nodes and edges from the python
        nodes = new vis.DataSet([{"color": "orange", "id": "doc2vec_1372", "label": "doc2vec_1372", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 1372 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2016.7883517\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Yuanzhe Chen;Qing Chen;Mingqian Zhao;Sebastien Boyer;Kalyan Veeramachaneni;Huamin Qu; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: DropoutSeer: Visualizing learning patterns in Massive Open Online Courses for dropout reasoning and prediction; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: Aiming at massive participation and open access education, Massive Open Online Courses (MOOCs) have attracted millions of learners over the past few years. However, the high dropout rate of learners is considered to be one of the most crucial factors that may hinder the development of MOOCs. To tackle this problem, statistical models have been developed to predict dropout behavior based on learner activity logs. Although predictive models can foresee the dropout behavior, it is still difficult for users to understand the reasons behind the predicted results and further design interventions to prevent dropout. In addition, with a better understanding of dropout, researchers in the area of predictive modeling in turn can improve the models. In this paper, we introduce DropoutSeer, a visual analytics system which not only helps instructors and education experts understand the reasons for dropout, but also allows researchers to identify crucial features which can further improve the performance of the models. Both the heterogeneous data extracted from three different kinds of learner activity logs (i.e., clickstream, forum posts and assignment records) and the predicted results are visualized in the proposed system. Case studies and expert interviews have been conducted to demonstrate the usefulness and effectiveness of DropoutSeer."}, {"color": "orange", "id": "reference_683", "label": "reference_683", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 683 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2011.6102447\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Yang Chen;Jamal Alsakran;Scott Barlowe;Jing Yang;Ye Zhao; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Supporting effective common ground construction in Asynchronous Collaborative Visual Analytics; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: Asynchronous Collaborative Visual Analytics (ACVA) leverages group sensemaking by releasing the constraints on when, where, and who works collaboratively. A significant task to be addressed before ACVA can reach its full potential is effective common ground construction, namely the process in which users evaluate insights from individual work to develop a shared understanding of insights and collectively pool them. This is challenging due to the lack of instant communication and scale of collaboration in ACVA. We propose a novel visual analytics approach that automatically gathers, organizes, and summarizes insights to form common ground with reduced human effort. The rich set of visualization and interaction techniques provided in our approach allows users to effectively and flexibly control the common ground construction and review, explore, and compare insights in detail. A working prototype of the approach has been implemented. We have conducted a case study and a user study to demonstrate its effectiveness."}, {"color": "orange", "id": "reference_423", "label": "reference_423", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 423 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2008.4677365\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: David Gotz;Michelle X. Zhou; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Characterizing users\u0027 visual analytic activity for insight provenance; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: Insight provenance - a historical record of the process and rationale by which an insight is derived - is an essential requirement in many visual analytics applications. While work in this area has relied on either manually recorded provenance (e.g., user notes) or automatically recorded event-based insight provenance (e.g., clicks, drags, and key-presses), both approaches have fundamental limitations. Our aim is to develop a new approach that combines the benefits of both approaches while avoiding their deficiencies. Toward this goal, we characterize userspsila visual analytic activity at multiple levels of granularity. Moreover, we identify a critical level of abstraction, Actions, that can be used to represent visual analytic activity with a set of general but semantically meaningful behavior types. In turn, the action types can be used as the semantic building blocks for insight provenance. We present a catalog of common actions identified through observations of several different visual analytic systems. In addition, we define a taxonomy to categorize actions into three major classes based on their semantic intent. The concept of actions has been integrated into our labpsilas prototype visual analytic system, HARVEST, as the basis for its insight provenance capabilities."}, {"color": "orange", "id": "doc2vec_502", "label": "doc2vec_502", "shape": "dot", "size": "35", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 502 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2009.5333023\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Yedendra B. Shrinivasan;David Gotzy;Jie Lu; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Connecting the dots in visual analysis; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: During visual analysis, users must often connect insights discovered at various points of time. This process is often called ldquoconnecting the dots.rdquo When analysts interactively explore complex datasets over multiple sessions, they may uncover a large number of findings. As a result, it is often difficult for them to recall the past insights, views and concepts that are most relevant to their current line of inquiry. This challenge is even more difficult during collaborative analysis tasks where they need to find connections between their own discoveries and insights found by others. In this paper, we describe a context-based retrieval algorithm to identify notes, views and concepts from users\u0027 past analyses that are most relevant to a view or a note based on their line of inquiry. We then describe a related notes recommendation feature that surfaces the most relevant items to the user as they work based on this algorithm. We have implemented this recommendation feature in HARVEST, a Web based visual analytic system. We evaluate the related notes recommendation feature of HARVEST through a case study and discuss the implications of our approach."}, {"color": "orange", "id": "reference_501", "label": "reference_501", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 501 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2009.5333020\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Nazanin Kadivar;Victor Chen;Dustin Dunsmuir;Eric Lee;Cheryl Qian;John Dill;Christopher Shaw;Robert Woodbury; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Capturing and supporting the analysis process; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: Visual analytics tools provide powerful visual representations in order to support the sense-making process. In this process, analysts typically iterate through sequences of steps many times, varying parameters each time. Few visual analytics tools support this process well, nor do they provide support for visualizing and understanding the analysis process itself. To help analysts understand, explore, reference, and reuse their analysis process, we present a visual analytics system named CzSaw (See-Saw) that provides an editable and re-playable history navigation channel in addition to multiple visual representations of document collections and the entities within them (in a manner inspired by Jigsaw). Conventional history navigation tools range from basic undo and redo to branching timelines of user actions. In CzSaw\u0027s approach to this, first, user interactions are translated into a script language that drives the underlying scripting-driven propagation system. The latter allows analysts to edit analysis steps, and ultimately to program them. Second, on this base, we build both a history view showing progress and alternative paths, and a dependency graph showing the underlying logic of the analysis and dependency relations among the results of each step. These tools result in a visual model of the sense-making process, providing a way for analysts to visualize their analysis process, to reinterpret the problem, explore alternative paths, extract analysis patterns from existing history, and reuse them with other related analyses."}, {"color": "orange", "id": "reference_1458", "label": "reference_1458", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 1458 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/TVCG.2017.2745180\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Darren Edge;Nathalie Henry Riche;Jonathan Larson;Christopher White; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Beyond Tasks: An Activity Typology for Visual Analytics; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: As Visual Analytics (VA) research grows and diversifies to encompass new systems, techniques, and use contexts, gaining a holistic view of analytic practices is becoming ever more challenging. However, such a view is essential for researchers and practitioners seeking to develop systems for broad audiences that span multiple domains. In this paper, we interpret VA research through the lens of Activity Theory (AT) - a framework for modelling human activities that has been influential in the field of Human-Computer Interaction. We first provide an overview of Activity Theory, showing its potential for thinking beyond tasks, representations, and interactions to the broader systems of activity in which interactive tools are embedded and used. Next, we describe how Activity Theory can be used as an organizing framework in the construction of activity typologies, building and expanding upon the tradition of abstract task taxonomies in the field of Information Visualization. We then apply the resulting process to create an activity typology for Visual Analytics, synthesizing a wide range of systems and activity concepts from the literature. Finally, we use this typology as the foundation of an activity-centered design process, highlighting both tensions and opportunities in the design space of VA systems."}, {"color": "orange", "id": "reference_502", "label": "reference_502", "shape": "dot", "size": "35", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 502 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2009.5333023\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Yedendra B. Shrinivasan;David Gotzy;Jie Lu; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Connecting the dots in visual analysis; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: During visual analysis, users must often connect insights discovered at various points of time. This process is often called ldquoconnecting the dots.rdquo When analysts interactively explore complex datasets over multiple sessions, they may uncover a large number of findings. As a result, it is often difficult for them to recall the past insights, views and concepts that are most relevant to their current line of inquiry. This challenge is even more difficult during collaborative analysis tasks where they need to find connections between their own discoveries and insights found by others. In this paper, we describe a context-based retrieval algorithm to identify notes, views and concepts from users\u0027 past analyses that are most relevant to a view or a note based on their line of inquiry. We then describe a related notes recommendation feature that surfaces the most relevant items to the user as they work based on this algorithm. We have implemented this recommendation feature in HARVEST, a Web based visual analytic system. We evaluate the related notes recommendation feature of HARVEST through a case study and discuss the implications of our approach."}, {"color": "blue", "id": "doc2vec_217", "label": "doc2vec_217", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 217 (InfoVis)  - \u003ca href=\"http://dx.doi.org/10.1109/INFVIS.2004.72\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: T.A. Keahey;K.C. Cox; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: VIM: A Framework for Intelligence Analysis; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: Intelligence analysts receive thousands of facts from a variety of sources. In addition to the bare details of the fact \u2014 a particular person, for example \u2014 each fact may have provenance, reliability, weight, and other attributes. Each fact may also be associated with other facts, e.g. that one person met another at a particular location. The analyst\u2019s task is to examine a huge collection of such loosely-structured facts, and try to \"connect the dots\" to perceive the underlying and unknown causes \u2014 and their possible future courses. We have designed and implemented a Java platform called VIM to support intelligence analysts in their work."}, {"color": "orange", "id": "reference_607", "label": "reference_607", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 607 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2010.5652879\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Narges Mahyar;Ali Sarvghad;Melanie Tory; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: A closer look at note taking in the co-located collaborative visual analytics process; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: This paper highlights the important role that record-keeping (i.e. taking notes and saving charts) plays in collaborative data analysis within the business domain. The discussion of record-keeping is based on observations from a user study in which co-located teams worked on collaborative visual analytics tasks using large interactive wall and tabletop displays. Part of our findings is a collaborative data analysis framework that encompasses note taking as one of the main activities. We observed that record-keeping was a critical activity within the analysis process. Based on our observations, we characterize notes according to their content, scope, and usage, and describe how they fit into a process of collaborative data analysis. We then discuss suggestions for the design of collaborative visual analytics tools."}, {"color": "orange", "id": "doc2vec_303", "label": "doc2vec_303", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 303 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2006.261439\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Susan E. Brennan;Klaus Mueller;Greg Zelinsky;IV Ramakrishnan;David S. Warren;Arie Kaufman; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Toward a Multi-Analyst, Collaborative Framework for Visual Analytics; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: We describe a framework for the display of complex, multidimensional data, designed to facilitate exploration, analysis, and collaboration among multiple analysts. This framework aims to support human collaboration by making it easier to share representations, to translate from one point of view to another, to explain arguments, to update conclusions when underlying assumptions change, and to justify or account for decisions or actions. Multidimensional visualization techniques are used with interactive, context-sensitive, and tunable graphs. Visual representations are flexibly generated using a knowledge representation scheme based on annotated logic; this enables not only tracking and fusing different viewpoints, but also unpacking them. Fusing representations supports the creation of multidimensional meta-displays as well as the translation or mapping from one point of view to another. At the same time, analysts also need to be able to unpack one another\u0027s complex chains of reasoning, especially if they have reached different conclusions, and to determine the implications, if any, when underlying assumptions or evidence turn out to be false. The framework enables us to support a variety of scenarios as well as to systematically generate and test experimental hypotheses about the impact of different kinds of visual representations upon interactive collaboration by teams of distributed analysts"}, {"color": "orange", "id": "reference_416", "label": "reference_416", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 416 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2008.4677358\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: Anthony C. Robinson; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Collaborative synthesis of visual analytic results; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: Visual analytic tools allow analysts to generate large collections of useful analytical results. We anticipate that analysts in most real world situations will draw from these collections when working together to solve complicated problems. This indicates a need to understand how users synthesize multiple collections of results. This paper reports the results of collaborative synthesis experiments conducted with expert geographers and disease biologists. Ten participants were worked in pairs to complete a simulated real-world synthesis task using artifacts printed on cards on a large, paper-covered workspace. Experiment results indicate that groups use a number of different approaches to collaborative synthesis, and that they employ a variety of organizational metaphors to structure their information. It is further evident that establishing common ground and role assignment are critical aspects of collaborative synthesis. We conclude with a set of general design guidelines for collaborative synthesis support tools."}, {"color": "orange", "id": "doc2vec_351", "label": "doc2vec_351", "shape": "dot", "title": "\u003cb\u003ePaper id\u003c/b\u003e: 351 (VAST)  - \u003ca href=\"http://dx.doi.org/10.1109/VAST.2007.4389006\" target=\"_blank\"\u003eLink\u003c/a\u003e; \u003cbr\u003e\u003cb\u003eAuthors: \u003c/b\u003e: John Stasko;Carsten Gorg;Zhicheng Liu;Kanupriya Singhal; \u003cbr\u003e\u003cb\u003eTitle\u003c/b\u003e: Jigsaw: Supporting Investigative Analysis through Interactive Visualization; \u003cbr\u003e\u003cb\u003eAbstract\u003c/b\u003e: Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents."}]);
        edges = new vis.DataSet([{"from": "doc2vec_217", "to": "doc2vec_502", "width": 0.86841875}, {"from": "doc2vec_303", "to": "doc2vec_502", "width": 0.89573365}, {"from": "doc2vec_351", "to": "doc2vec_502", "width": 0.86190736}, {"from": "doc2vec_502", "to": "doc2vec_1372", "width": 0.87894803}, {"from": "reference_416", "to": "reference_423", "width": 4.0}, {"from": "reference_416", "to": "reference_502", "width": 4.0}, {"from": "reference_423", "to": "reference_501", "width": 9.0}, {"from": "reference_423", "to": "reference_502", "width": 4.0}, {"from": "reference_423", "to": "reference_607", "width": 4.0}, {"from": "reference_423", "to": "reference_1458", "width": 4.0}, {"from": "reference_501", "to": "reference_502", "width": 5.0}, {"from": "reference_501", "to": "reference_1458", "width": 4.0}, {"from": "reference_502", "to": "reference_607", "width": 4.0}, {"from": "reference_502", "to": "reference_683", "width": 4.0}, {"from": "reference_502", "to": "reference_1458", "width": 4.0}, {"from": "reference_607", "to": "reference_683", "width": 5.0}]);

        // adding nodes and edges to the graph
        data = {nodes: nodes, edges: edges};

        var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};
        
        

        

        network = new vis.Network(container, data, options);
	 
        
        // make a custom popup
        var popup = document.createElement("div");
        popup.className = 'popup';
        popupTimeout = null;
        popup.addEventListener('mouseover', function () {
            console.log(popup)
            if (popupTimeout !== null) {
                clearTimeout(popupTimeout);
                popupTimeout = null;
            }
        });
        popup.addEventListener('mouseout', function () {
            if (popupTimeout === null) {
                hidePopup();
            }
        });
        container.appendChild(popup);


        // use the popup event to show
        network.on("showPopup", function (params) {
            showPopup(params);
        });

        // use the hide event to hide it
        network.on("hidePopup", function (params) {
            hidePopup();
        });


        // hiding the popup through css
        function hidePopup() {
            popupTimeout = setTimeout(function () { popup.style.display = 'none'; }, 500);
        }

        // showing the popup
        function showPopup(nodeId) {
            // get the data from the vis.DataSet
            var nodeData = nodes.get([nodeId]);
            popup.innerHTML = nodeData[0].title;

            // get the position of the node
            var posCanvas = network.getPositions([nodeId])[nodeId];

            // get the bounding box of the node
            var boundingBox = network.getBoundingBox(nodeId);

            //position tooltip:
            posCanvas.x = posCanvas.x + 0.5 * (boundingBox.right - boundingBox.left);

            // convert coordinates to the DOM space
            var posDOM = network.canvasToDOM(posCanvas);

            // Give it an offset
            posDOM.x += 10;
            posDOM.y -= 20;

            // show and place the tooltip.
            popup.style.display = 'block';
            popup.style.top = posDOM.y + 'px';
            popup.style.left = posDOM.x + 'px';
        }
        


        return network;

    }

    drawGraph();

</script>
</body>
</html>